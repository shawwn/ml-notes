{
    "iterations": 20,
    "batch_per_core": 9,
    "precision": "float32",
    "n_head": 12,
    "encoder_path": "encoder",
    "n_vocab": 50257,
    "embed_dropout": 0.0,
    "lr": 0.00025,
    "warmup_steps": 0,
    "beta1": 0.9,
    "beta2": 0.999,
    "epsilon": 1e-9,
    "opt_name": "adam",
    "train_batch_size": 1024,
    "attn_dropout": 0.0,
    "train_steps": -1,
    "eval_steps": 10,
    "max_steps": 5000000,
    "res_dropout": 0.0,
    "predict_batch_size": 1,
    "eval_batch_size": 32,
    "n_embd": 768,
    "input": "my_input",
    "model": "GPT2",
    "n_ctx": 1024,
    "predict_path": "logs/predictions.txt",
    "n_layer": 12,
    "scale_by_depth": false,
    "scale_by_in": false
}
